{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_training.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93wHXFbkF7Kv"
      },
      "source": [
        "required package installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-g54myBFupy"
      },
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()\n",
        "!conda install -c conda-forge iris"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeZaVrNgGF40"
      },
      "source": [
        "Model and training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzsnL0tUGJTs"
      },
      "source": [
        "# MODEL ARCHITECTURE TAKEN FROM  \"https://github.com/lixiaolei1982/Keras-Implementation-of-U-Net-R2U-Net-Attention-U-Net-Attention-R2U-Net.-\"\n",
        "import iris\n",
        "import iris.analysis.cartography\n",
        "import iris.quickplot as qplt\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import datetime\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import multiply,add,Conv2D, Conv3D, MaxPooling2D, MaxPooling3D, UpSampling2D, UpSampling3D, Add, BatchNormalization, Input, Activation, Lambda, Cropping2D,Concatenate,Dropout\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorboard.plugins.hparams import api as hp\n",
        "\n",
        "import glob\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint,EarlyStopping,TensorBoard\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "config = tf.compat.v1.ConfigProto() \n",
        "config.gpu_options.allow_growth = True\n",
        "session = tf.compat.v1.Session(config=config)\n",
        "\n",
        "\n",
        "def unet(img_w, img_h,channel, n_label=1, data_format='channels_last'):\n",
        "    inputs = Input(( img_w, img_h,channel),name='inputs')\n",
        "    x = inputs\n",
        "    depth = 3\n",
        "    features = 64\n",
        "    skips = []\n",
        "    for i in range(depth):\n",
        "        x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n",
        "        x = Dropout(0.2)(x)\n",
        "        x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n",
        "        skips.append(x)\n",
        "        x = MaxPooling2D((2, 2), data_format= data_format)(x)\n",
        "        features = features * 2\n",
        "\n",
        "    x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n",
        "\n",
        "    for i in reversed(range(depth)):\n",
        "        features = features // 2\n",
        "        # attention_up_and_concate(x,[skips[i])\n",
        "        x = UpSampling2D(size=(2, 2), data_format=data_format)(x)\n",
        "        x = K.concatenate([skips[i], x], axis=3)\n",
        "        x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n",
        "        x = Dropout(0.2)(x)\n",
        "        x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n",
        "\n",
        "    conv6 = Conv2D(n_label, (1, 1), padding='same', data_format=data_format)(x)\n",
        "    conv7 = Activation('linear',name='outputs')(conv6)\n",
        "    model = Model(inputs=inputs, outputs=conv7)\n",
        "\n",
        "    #model.compile(optimizer=Adam(lr=1e-5), loss=[focal_loss()], metrics=['accuracy', dice_coef])\n",
        "    return model\n",
        "########################################################################################################\n",
        "#Attention U-Net\n",
        "def att_unet(img_w, img_h,channel, n_label=1, data_format='channels_last'):\n",
        "    inputs = Input(( img_w, img_h,channel),name='inputs')\n",
        "    x = inputs\n",
        "    depth = 3\n",
        "    features = 64\n",
        "    skips = []\n",
        "    for i in range(depth):\n",
        "        x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n",
        "        x = Dropout(0.2)(x)\n",
        "        x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n",
        "        skips.append(x)\n",
        "        x = MaxPooling2D((2, 2), data_format=data_format)(x)\n",
        "        features = features * 2\n",
        "\n",
        "    x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n",
        "\n",
        "    for i in reversed(range(depth)):\n",
        "        features = features // 2\n",
        "        x = attention_up_and_concate(x, skips[i], data_format=data_format)\n",
        "        x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n",
        "        x = Dropout(0.2)(x)\n",
        "        x = Conv2D(features, (3, 3), activation='relu', padding='same', data_format=data_format)(x)\n",
        "\n",
        "    conv6 = Conv2D(n_label, (1, 1), padding='same', data_format=data_format)(x)\n",
        "    conv7 = Activation('linear',name='outputs')(conv6)\n",
        "    model = Model(inputs=inputs, outputs=conv7)\n",
        "\n",
        "    #model.compile(optimizer=Adam(lr=1e-5), loss=[focal_loss()], metrics=['accuracy', dice_coef])\n",
        "    return model\n",
        "\n",
        "\n",
        "def rec_res_block(input_layer, out_n_filters, batch_normalization=False, kernel_size=[3, 3], stride=[1, 1],\n",
        "\n",
        "                  padding='same', data_format='channels_first'):\n",
        "    if data_format == 'channels_first':\n",
        "        input_n_filters = input_layer.get_shape().as_list()[1]\n",
        "    else:\n",
        "        input_n_filters = input_layer.get_shape().as_list()[3]\n",
        "\n",
        "    if out_n_filters != input_n_filters:\n",
        "        skip_layer = Conv2D(out_n_filters, [1, 1], strides=stride, padding=padding, data_format=data_format)(\n",
        "            input_layer)\n",
        "    else:\n",
        "        skip_layer = input_layer\n",
        "\n",
        "    layer = skip_layer\n",
        "    for j in range(2):\n",
        "\n",
        "        for i in range(2):\n",
        "            if i == 0:\n",
        "\n",
        "                layer1 = Conv2D(out_n_filters, kernel_size, strides=stride, padding=padding, data_format=data_format)(\n",
        "                    layer)\n",
        "                if batch_normalization:\n",
        "                    layer1 = BatchNormalization()(layer1)\n",
        "                layer1 = Activation('relu')(layer1)\n",
        "            layer1 = Conv2D(out_n_filters, kernel_size, strides=stride, padding=padding, data_format=data_format)(\n",
        "                add([layer1, layer]))\n",
        "            if batch_normalization:\n",
        "                layer1 = BatchNormalization()(layer1)\n",
        "            layer1 = Activation('relu')(layer1)\n",
        "        layer = layer1\n",
        "\n",
        "    out_layer = add([layer, skip_layer])\n",
        "    return out_layer\n",
        "\n",
        "\n",
        "def attention_up_and_concate(down_layer, layer, data_format='channels_first'):\n",
        "    if data_format == 'channels_first':\n",
        "        in_channel = down_layer.get_shape().as_list()[1]\n",
        "    else:\n",
        "        in_channel = down_layer.get_shape().as_list()[3]\n",
        "\n",
        "    # up = Conv2DTranspose(out_channel, [2, 2], strides=[2, 2])(down_layer)\n",
        "    up = UpSampling2D(size=(2, 2), data_format=data_format)(down_layer)\n",
        "\n",
        "    layer = attention_block_2d(x=layer, g=up, inter_channel=in_channel // 4, data_format=data_format)\n",
        "\n",
        "    if data_format == 'channels_first':\n",
        "        my_concat = Lambda(lambda x: K.concatenate([x[0], x[1]], axis=1))\n",
        "    else:\n",
        "        my_concat = Lambda(lambda x: K.concatenate([x[0], x[1]], axis=3))\n",
        "\n",
        "    concate = my_concat([up, layer])\n",
        "    return concate\n",
        "def attention_block_2d(x, g, inter_channel, data_format='channels_first'):\n",
        "    # theta_x(?,g_height,g_width,inter_channel)\n",
        "\n",
        "    theta_x = Conv2D(inter_channel, [1, 1], strides=[1, 1], data_format=data_format)(x)\n",
        "\n",
        "    # phi_g(?,g_height,g_width,inter_channel)\n",
        "\n",
        "    phi_g = Conv2D(inter_channel, [1, 1], strides=[1, 1], data_format=data_format)(g)\n",
        "\n",
        "    # f(?,g_height,g_width,inter_channel)\n",
        "\n",
        "    f = Activation('relu')(add([theta_x, phi_g]))\n",
        "\n",
        "    # psi_f(?,g_height,g_width,1)\n",
        "\n",
        "    psi_f = Conv2D(1, [1, 1], strides=[1, 1], data_format=data_format)(f)\n",
        "\n",
        "    rate = Activation('sigmoid')(psi_f)\n",
        "\n",
        "    # rate(?,x_height,x_width)\n",
        "\n",
        "    # att_x(?,x_height,x_width,x_channel)\n",
        "\n",
        "    att_x = multiply([x, rate])\n",
        "\n",
        "    return att_x\n",
        "def att_r2_unet(img_w, img_h,channel=2, n_label=1, data_format='channels_last'):\n",
        "    inputs = Input(( img_w, img_h,channel),name='inputs')\n",
        "    x = inputs\n",
        "    depth =3\n",
        "    features = 64\n",
        "    skips = []\n",
        "    for i in range(depth):\n",
        "        x = rec_res_block(x, features, data_format=data_format)\n",
        "        skips.append(x)\n",
        "        x = MaxPooling2D((2, 2), data_format=data_format)(x)\n",
        "\n",
        "        features = features * 2\n",
        "\n",
        "    x = rec_res_block(x, features, data_format=data_format)\n",
        "\n",
        "    for i in reversed(range(depth)):\n",
        "        features = features // 2\n",
        "        x = attention_up_and_concate(x, skips[i], data_format=data_format)\n",
        "        x = rec_res_block(x, features, data_format=data_format)\n",
        "\n",
        "    conv6 = Conv2D(n_label, (1, 1), padding='same', data_format=data_format)(x)\n",
        "    conv7 = Activation('linear',name='outputs')(conv6)\n",
        "    model = Model(inputs=inputs, outputs=conv7)\n",
        "    #model.compile(optimizer=Adam(lr=1e-6), loss=[dice_coef_loss], metrics=['accuracy', dice_coef])\n",
        "    return model\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "config = tf.compat.v1.ConfigProto() \n",
        "config.gpu_options.allow_growth = True\n",
        "session = tf.compat.v1.Session(config=config)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "gpp_test_min=0\n",
        "gpp_test_max=0\n",
        "\n",
        "cubes_tas = []\n",
        "def getData(dir,time_constraint_train,time_constraint_test):\n",
        "  list1 = []\n",
        "\n",
        "\n",
        "  #dir = '/content/drive/MyDrive/U-Birmingham-gpp-data/annual/*CMIP6_BCC*.nc'\n",
        "\n",
        "  \n",
        "\n",
        "  for name in glob.glob(dir):\n",
        "      list1.append(name)\n",
        "\n",
        "  cubes_tas = iris.load(list1)\n",
        "\n",
        "  # equalise_attributes(cubes_tas)\n",
        "  # unify_time_units(cubes_tas)\n",
        "  # names = [cube.metadata.name() for cube in cubes ]\n",
        "  # unique_names = list(OrderedDict.fromkeys(names))\n",
        "\n",
        "\n",
        "  for z in cubes_tas:\n",
        "      try:\n",
        "\n",
        "          if (z.coord('season_year')):\n",
        "              year = z.coord('season_year')\n",
        "              # Fix the names. Latitude values, units and\n",
        "              year.var_name = 'year'\n",
        "              year.long_name = 'year'\n",
        "      except:\n",
        "          print(\"An exception occurred aux\")\n",
        "\n",
        "  # Fix the names. Latitude values, units and\n",
        "\n",
        "\n",
        "  for z in cubes_tas:\n",
        "      try:\n",
        "          z.remove_coord(\"clim_season\")\n",
        "      except:\n",
        "          print(\"An exception occurred remove cilmate\")\n",
        "\n",
        "  for z in cubes_tas:\n",
        "      try:\n",
        "          if (z._names[0] == 'precipitation_flux'):\n",
        "              z.convert_units('kg m-2 year-1')\n",
        "          if (z._names[0] == 'air_temperature'):\n",
        "              z.convert_units('celsius')\n",
        "          if (z._names[0] == 'gross_primary_productivity_of_biomass_expressed_as_carbon'):\n",
        "              z.convert_units('kg m-2 year-1')\n",
        "\n",
        "      except:\n",
        "          print(\"An exception occurred conversion\")\n",
        "\n",
        "  #time_constraint_train = iris.Constraint(year=lambda cell: 1900 < cell < 2300)\n",
        "  #time_constraint_test = iris.Constraint(year=lambda cell: 1860 < cell < 1900)\n",
        "\n",
        "  tas_array_train = []\n",
        "  pr_array_train = []\n",
        "  gpp_array_train = []\n",
        "\n",
        "  tas_array_test = []\n",
        "  pr_array_test = []\n",
        "  gpp_array_test = []\n",
        "  mask_train = []\n",
        "  mask_test=[]\n",
        "\n",
        "  for z in cubes_tas:\n",
        "      try:\n",
        "          if (z._names[0] == 'precipitation_flux'):\n",
        "              temp = z.extract(time_constraint_train)\n",
        "              mask_train.append(temp.data._mask)\n",
        "              pr_array_train.append(temp.data.filled(0))\n",
        "              temp = z.extract(time_constraint_test)\n",
        "              mask_test.append(temp.data._mask)             \n",
        "              pr_array_test.append(temp.data.filled(0))             \n",
        "          if (z._names[0] == 'air_temperature'):\n",
        "              temp = z.extract(time_constraint_train)\n",
        "              tas_array_train.append(temp.data.filled(0))\n",
        "              temp = z.extract(time_constraint_test)\n",
        "              tas_array_test.append(temp.data.filled(0))\n",
        "          if (z._names[0] == 'gross_primary_productivity_of_biomass_expressed_as_carbon'):\n",
        "              temp = z.extract(time_constraint_train)\n",
        "              gpp_array_train.append(temp.data.filled(0))\n",
        "              temp = z.extract(time_constraint_test)\n",
        "              gpp_array_test.append(temp.data.filled(0))\n",
        "\n",
        "      except:\n",
        "          print(\"An exception occurred  filling data\")\n",
        "\n",
        "  tas_array_train = np.concatenate(tas_array_train)\n",
        "  pr_array_train = np.concatenate(pr_array_train)\n",
        "\n",
        "\n",
        "  gpp_array_train = np.concatenate(gpp_array_train)\n",
        "\n",
        "  gpp_array_train = np.subtract(gpp_array_train, gpp_array_train.min()) / np.subtract(gpp_array_train.max(),\n",
        "                                                                                      gpp_array_train.min())\n",
        "\n",
        "  pr_array_train = np.subtract(pr_array_train, pr_array_train.min()) / np.subtract(pr_array_train.max(),\n",
        "                                                                                  pr_array_train.min())\n",
        "\n",
        "  tas_array_train = np.subtract(tas_array_train, tas_array_train.min()) / np.subtract(tas_array_train.max(),\n",
        "                                                                                      tas_array_train.min())\n",
        "\n",
        "  mask_train = np.concatenate(mask_train)\n",
        "\n",
        "  tas_array_test = np.concatenate(tas_array_test)\n",
        "  print(gpp_array_test)\n",
        "  pr_array_test = np.concatenate(pr_array_test)\n",
        "  gpp_array_test = np.concatenate(gpp_array_test)\n",
        "\n",
        "  tas_array_test = np.subtract(tas_array_test, tas_array_test.min()) / np.subtract(tas_array_test.max(),\n",
        "                                                                                  tas_array_test.min())\n",
        "\n",
        "  pr_array_test = np.subtract(pr_array_test, pr_array_test.min()) / np.subtract(pr_array_test.max(), pr_array_test.min())\n",
        "\n",
        "  gpp_test_min = gpp_array_test.min()\n",
        "  gpp_test_max = gpp_array_test.max()\n",
        "\n",
        "  gpp_array_test = np.subtract(gpp_array_test, gpp_array_test.min()) / np.subtract(gpp_array_test.max(),\n",
        "                                                                                  gpp_array_test.min())\n",
        "\n",
        "  mask_test = np.concatenate(mask_test)\n",
        "  train_data = np.stack([tas_array_train,pr_array_train], axis=-1)\n",
        "\n",
        "  test_data = np.stack([tas_array_test,pr_array_test], axis=-1)\n",
        "\n",
        "  return train_data,test_data,gpp_array_test,gpp_array_train,mask_test,cubes_tas\n",
        "\n",
        "\n",
        "def train_model(train_data,test_data,gpp_array_test,gpp_array_train,dir,model_name,model):\n",
        "  \n",
        "  if(model=='unet'):\n",
        "    model=unet(test_data.shape[1],test_data.shape[2],test_data.shape[3])\n",
        "  elif(model=='att_unet'):\n",
        "    model=att_unet(test_data.shape[1],test_data.shape[2],test_data.shape[3])\n",
        "  elif(model=='att_r2_unet'):\n",
        "    model=att_r2_unet(test_data.shape[1],test_data.shape[2],test_data.shape[3])\n",
        "  \n",
        "  \n",
        "  \n",
        "  model.summary()\n",
        "  model.compile(optimizer=Adam(learning_rate=0.0001),\n",
        "                loss=tf.keras.losses.MeanAbsoluteError(),\n",
        "                )\n",
        "\n",
        "  \n",
        "  log_dir = dir+\"/log1/\" + model_name+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  callbacks = [\n",
        "      ModelCheckpoint(\n",
        "          # Path where to save the model\n",
        "          # The two parameters below mean that we will overwrite\n",
        "          # the current checkpoint if and only if\n",
        "          # the `val_loss` score has improved.\n",
        "          # The saved model name will include the current epoch.\n",
        "          filepath=dir+model_name,\n",
        "          save_best_only=True,  # Only save a model if `val_loss` has improved.\n",
        "          monitor=\"val_loss\",\n",
        "          verbose=1,\n",
        "      ),\n",
        "      EarlyStopping(\n",
        "          # Stop training when `val_loss` is no longer improving\n",
        "          monitor=\"val_loss\",\n",
        "          # \"no longer improving\" being defined as \"no better than 1e-2 less\"\n",
        "          min_delta=1e-3,\n",
        "          # \"no longer improving\" being further defined as \"for at least 2 epochs\"\n",
        "          patience=25,\n",
        "          verbose=1,\n",
        "      ),\n",
        "      TensorBoard(\n",
        "      log_dir=log_dir,\n",
        "      histogram_freq=1,  # How often to log histogram visualizations\n",
        "      embeddings_freq=1,  # How often to log embedding visualizations\n",
        "      update_freq=\"epoch\",\n",
        "      )\n",
        "  ]\n",
        "\n",
        "\n",
        "  model.fit(\n",
        "      {\"inputs\":train_data,},\n",
        "      {\"outputs\": gpp_array_train },\n",
        "      batch_size=32,\n",
        "      epochs=200,\n",
        "      callbacks=callbacks,\n",
        "      validation_split=0.1\n",
        "  )\n",
        "\n",
        "\n",
        "\n",
        "def test_model(train_data,test_data,gpp_array_test,gpp_array_train,model_dir,plot_dir,cubes_tas):\n",
        "  import matplotlib.cm as mpl_cm\n",
        "  model = tf.keras.models.load_model(model_dir)\n",
        "\n",
        "  z = model({\"inputs\": test_data})\n",
        "  val = z.numpy()\n",
        "  val1 = np.reshape(val, (test_data.shape[0], test_data.shape[1], test_data.shape[2]))\n",
        "  finale_array = val1 * gpp_test_max\n",
        "\n",
        "  import numpy.ma as ma\n",
        "  finale_array[finale_array<0]=0\n",
        "  finale_array=ma.masked_array(finale_array,mask_test)\n",
        "  brewer_cmap = mpl_cm.get_cmap('brewer_PiYG_11')\n",
        "\n",
        "\n",
        "\n",
        "  start=0\n",
        "  input_count=1\n",
        "  outputcount=1\n",
        "  for z in cubes_tas:\n",
        "      # try:\n",
        "          if (z._names[0] == 'gross_primary_productivity_of_biomass_expressed_as_carbon'):\n",
        "              temp = z.extract(time_constraint_test)\n",
        "              temp1=temp.copy()\n",
        "              temp1.data=finale_array[start:(start+temp.shape[0]),:,:]\n",
        "              start+=temp.shape[0]\n",
        "              for x,y in zip(temp.slices_over('time'),temp1.slices_over('time')):\n",
        "                fig = plt.figure(figsize=(20, 5),dpi=200)\n",
        "                plt.subplot(131)\n",
        "                qplt.contourf(x,brewer_cmap.N, cmap=brewer_cmap)\n",
        "                plt.title('GPP  '+str(x.coord('year').points[0]))\n",
        "                \n",
        "                \n",
        "                #plt.show()\n",
        "                plt.subplot(132)\n",
        "                qplt.contourf(y,brewer_cmap.N, cmap=brewer_cmap)\n",
        "                plt.title('Predicted GPP '+str(x.coord('year').points[0]))\n",
        "                \n",
        "                \n",
        "\n",
        "                plt.subplot(133)\n",
        "                qplt.contourf(x-y,brewer_cmap.N, cmap=brewer_cmap)\n",
        "                plt.title('Difference'+str(x.coord('year').points[0]))\n",
        "                #plt.show()\n",
        "                plt.savefig(plot_dir+'Plot'+str(input_count) )\n",
        "                plt.close('all')\n",
        "                input_count+=1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "\n",
        "run=3\n",
        "\n",
        "time_constraint_train = iris.Constraint(year=lambda cell: 2000 < cell < 2300)\n",
        "time_constraint_test = iris.Constraint(year=lambda cell: 1960 < cell < 2000)\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/U-Birmingham-gpp-data/annual/*BCC*.nc'\n",
        "train_data,test_data,gpp_array_test,gpp_array_train,mask_test,cubes_tas=getData(data_dir,time_constraint_train,time_constraint_test)\n",
        "model_dir=\"/content/drive/MyDrive/models_new/\"\n",
        "model_name='annual_bcc_r2_unet'\n",
        "train_model(train_data,test_data,gpp_array_test,gpp_array_train,model_dir,model_name,'att_r2_unet')\n",
        "model_name='annual_bcc_unet'\n",
        "train_model(train_data,test_data,gpp_array_test,gpp_array_train,model_dir,model_name,'unet')\n",
        "model_name='annual_bcc_att_unet'\n",
        "train_model(train_data,test_data,gpp_array_test,gpp_array_train,model_dir,model_name,'att_unet')\n",
        "\n",
        "# model_dir=dir+model_name\n",
        "# plot_dir='/content/drive/MyDrive/plots/annual_ukesm/run'+str(run)+'/'\n",
        "# Path(plot_dir).mkdir(parents=True, exist_ok=True)\n",
        "# test_model(train_data,test_data,gpp_array_test,gpp_array_train,model_dir,plot_dir,cubes_tas)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "dir = '/content/drive/MyDrive/U-Birmingham-gpp-data/jja/*BCC*.nc'\n",
        "train_data,test_data,gpp_array_test,gpp_array_train,mask_test,cubes_tas=getData(data_dir,time_constraint_train,time_constraint_test)\n",
        "model_dir=\"/content/drive/MyDrive/models_new/\"\n",
        "model_name='jja_bcc_r2_unet'\n",
        "train_model(train_data,test_data,gpp_array_test,gpp_array_train,model_dir,model_name,'att_r2_unet')\n",
        "model_name='jja_bcc_unet'\n",
        "train_model(train_data,test_data,gpp_array_test,gpp_array_train,model_dir,model_name,'unet')\n",
        "model_name='jja_bcc_att_unet'\n",
        "train_model(train_data,test_data,gpp_array_test,gpp_array_train,model_dir,model_name,'att_unet')\n",
        "\n",
        "# model_dir=dir+model_name\n",
        "# plot_dir='/content/drive/MyDrive/plots/jja_ukesm/run'+str(run)+'/'\n",
        "# Path(plot_dir).mkdir(parents=True, exist_ok=True)\n",
        "# test_model(train_data,test_data,gpp_array_test,gpp_array_train,model_dir,plot_dir,cubes_tas)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "dir = '/content/drive/MyDrive/U-Birmingham-gpp-data/djf/*BCC*.nc'\n",
        "train_data,test_data,gpp_array_test,gpp_array_train,mask_test,cubes_tas=getData(data_dir,time_constraint_train,time_constraint_test)\n",
        "dir=\"/content/drive/MyDrive/models_new/\"\n",
        "model_dir=\"/content/drive/MyDrive/models_new/\"\n",
        "model_name='djf_bcc_r2_unet'\n",
        "train_model(train_data,test_data,gpp_array_test,gpp_array_train,model_dir,model_name,'att_r2_unet')\n",
        "model_name='djf_bcc_unet'\n",
        "train_model(train_data,test_data,gpp_array_test,gpp_array_train,model_dir,model_name,'unet')\n",
        "model_name='djf_bcc_att_unet'\n",
        "train_model(train_data,test_data,gpp_array_test,gpp_array_train,model_dir,model_name,'att_unet')\n",
        "\n",
        "# model_dir=dir+model_name\n",
        "\n",
        "# plot_dir='/content/drive/MyDrive/plots/djf_ukesh/run'+str(run)+'/'\n",
        "# Path(plot_dir).mkdir(parents=True, exist_ok=True)\n",
        "# test_model(train_data,test_data,gpp_array_test,gpp_array_train,model_dir,plot_dir,cubes_tas)\n",
        "\n",
        "\n",
        "time_constraint_train = iris.Constraint(year=lambda cell: 2000 < cell < 3059)\n",
        "time_constraint_test = iris.Constraint(year=lambda cell: 1960 < cell < 2000)\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/U-Birmingham-gpp-data/annual/*UKE*.nc'\n",
        "train_data,test_data,gpp_array_test,gpp_array_train,mask_test,cubes_tas=getData(data_dir,time_constraint_train,time_constraint_test)\n",
        "model_dir=\"/content/drive/MyDrive/models_new/\"\n",
        "model_name='annual_bcc_r2_unet'\n",
        "train_model(train_data,test_data,gpp_array_test,gpp_array_train,model_dir,model_name,'att_r2_unet')\n",
        "model_name='annual_bcc_unet'\n",
        "train_model(train_data,test_data,gpp_array_test,gpp_array_train,model_dir,model_name,'unet')\n",
        "model_name='annual_bcc_att_unet'\n",
        "train_model(train_data,test_data,gpp_array_test,gpp_array_train,model_dir,model_name,'att_unet')\n",
        "\n",
        "# model_dir=dir+model_name\n",
        "# plot_dir='/content/drive/MyDrive/plots/annual_ukesm/run'+str(run)+'/'\n",
        "# Path(plot_dir).mkdir(parents=True, exist_ok=True)\n",
        "# test_model(train_data,test_data,gpp_array_test,gpp_array_train,model_dir,plot_dir,cubes_tas)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "dir = '/content/drive/MyDrive/U-Birmingham-gpp-data/jja/*UKE*.nc'\n",
        "train_data,test_data,gpp_array_test,gpp_array_train,mask_test,cubes_tas=getData(data_dir,time_constraint_train,time_constraint_test)\n",
        "model_dir=\"/content/drive/MyDrive/models_new/\"\n",
        "model_name='jja_bcc_r2_unet'\n",
        "train_model(train_data,test_data,gpp_array_test,gpp_array_train,model_dir,model_name,'att_r2_unet')\n",
        "model_name='jja_bcc_unet'\n",
        "train_model(train_data,test_data,gpp_array_test,gpp_array_train,model_dir,model_name,'unet')\n",
        "model_name='jja_bcc_att_unet'\n",
        "train_model(train_data,test_data,gpp_array_test,gpp_array_train,model_dir,model_name,'att_unet')\n",
        "\n",
        "# model_dir=dir+model_name\n",
        "# plot_dir='/content/drive/MyDrive/plots/jja_ukesm/run'+str(run)+'/'\n",
        "# Path(plot_dir).mkdir(parents=True, exist_ok=True)\n",
        "# test_model(train_data,test_data,gpp_array_test,gpp_array_train,model_dir,plot_dir,cubes_tas)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "dir = '/content/drive/MyDrive/U-Birmingham-gpp-data/djf/*UKE*.nc'\n",
        "train_data,test_data,gpp_array_test,gpp_array_train,mask_test,cubes_tas=getData(data_dir,time_constraint_train,time_constraint_test)\n",
        "dir=\"/content/drive/MyDrive/models_new/\"\n",
        "model_dir=\"/content/drive/MyDrive/models_new/\"\n",
        "model_name='djf_bcc_r2_unet'\n",
        "train_model(train_data,test_data,gpp_array_test,gpp_array_train,model_dir,model_name,'att_r2_unet')\n",
        "model_name='djf_bcc_unet'\n",
        "train_model(train_data,test_data,gpp_array_test,gpp_array_train,model_dir,model_name,'unet')\n",
        "model_name='djf_bcc_att_unet'\n",
        "train_model(train_data,test_data,gpp_array_test,gpp_array_train,model_dir,model_name,'att_unet')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVn-KxnDGtil"
      },
      "source": [
        "Code to test on saved  model and estimate gpp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyL4I-8JGJZX"
      },
      "source": [
        "import iris\n",
        "import iris.analysis.cartography\n",
        "import iris.quickplot as qplt\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import datetime\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import multiply,add,Conv2D, Conv3D, MaxPooling2D, MaxPooling3D, UpSampling2D, UpSampling3D, Add, BatchNormalization, Input, Activation, Lambda, Cropping2D,Concatenate\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorboard.plugins.hparams import api as hp\n",
        "\n",
        "import glob\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint,EarlyStopping,TensorBoard\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "list1 = []\n",
        "\n",
        "run=5\n",
        "dir = '/content/drive/MyDrive/U-Birmingham-gpp-data/annual/*UKE*.nc'\n",
        "model_name='annual_ukesm'\n",
        "model_path='/content/drive/MyDrive/models/'+model_name\n",
        "# for name in glob.glob(dir):\n",
        "#     list1.append(name)\n",
        "\n",
        "from pathlib import Path\n",
        "plot_dir='/content/drive/MyDrive/plots/'+model_name+'/run'+str(run)+'/'\n",
        "Path(plot_dir).mkdir(parents=True, exist_ok=True)\n",
        "cubes_tas = []\n",
        "for name in glob.glob(dir):\n",
        "    list1.append(name)\n",
        "\n",
        "cubes_tas = iris.load(list1)\n",
        "\n",
        "print(list1)\n",
        "# equalise_attributes(cubes_tas)\n",
        "# unify_time_units(cubes_tas)\n",
        "# names = [cube.metadata.name() for cube in cubes ]\n",
        "# unique_names = list(OrderedDict.fromkeys(names))\n",
        "\n",
        "\n",
        "for z in cubes_tas:\n",
        "    try:\n",
        "\n",
        "        if (z.coord('season_year')):\n",
        "            year = z.coord('season_year')\n",
        "            # Fix the names. Latitude values, units and\n",
        "            year.var_name = 'year'\n",
        "            year.long_name = 'year'\n",
        "    except:\n",
        "        print(\"An exception occurred\")\n",
        "\n",
        "# Fix the names. Latitude values, units and\n",
        "\n",
        "\n",
        "for z in cubes_tas:\n",
        "    try:\n",
        "        z.remove_coord(\"clim_season\")\n",
        "    except:\n",
        "        print(\"An exception occurred\")\n",
        "\n",
        "for z in cubes_tas:\n",
        "    try:\n",
        "        if (z._names[0] == 'precipitation_flux'):\n",
        "            z.convert_units('kg m-2 days-1')\n",
        "        if (z._names[0] == 'air_temperature'):\n",
        "            z.convert_units('celsius')\n",
        "        if (z._names[0] == 'gross_primary_productivity_of_biomass_expressed_as_carbon'):\n",
        "            z.convert_units('kg m-2 year-1')\n",
        "\n",
        "    except:\n",
        "        print(\"An exception occurred\")\n",
        "\n",
        "time_constraint_train = iris.Constraint(year=lambda cell: 2000 < cell < 3049)\n",
        "time_constraint_test = iris.Constraint(year=lambda cell: 1960 < cell < 2000)\n",
        "\n",
        "tas_array_train = []\n",
        "pr_array_train = []\n",
        "gpp_array_train = []\n",
        "\n",
        "tas_array_test = []\n",
        "pr_array_test = []\n",
        "gpp_array_test = []\n",
        "mask_train = []\n",
        "mask_test = []\n",
        "\n",
        "for z in cubes_tas:\n",
        "    try:\n",
        "        if (z._names[0] == 'precipitation_flux'):\n",
        "            temp = z.extract(time_constraint_train)\n",
        "            mask_train.append(temp.data._mask)\n",
        "            pr_array_train.append(temp.data.filled(0))\n",
        "            temp = z.extract(time_constraint_test)\n",
        "            mask_test.append(temp.data._mask)\n",
        "            pr_array_test.append(temp.data.filled(0))\n",
        "        if (z._names[0] == 'air_temperature'):\n",
        "            temp = z.extract(time_constraint_train)\n",
        "            tas_array_train.append(temp.data.filled(0))\n",
        "            temp = z.extract(time_constraint_test)\n",
        "            tas_array_test.append(temp.data.filled(0))\n",
        "        if (z._names[0] == 'gross_primary_productivity_of_biomass_expressed_as_carbon'):\n",
        "            temp = z.extract(time_constraint_train)\n",
        "            gpp_array_train.append(temp.data.filled(0))\n",
        "            temp = z.extract(time_constraint_test)\n",
        "            gpp_array_test.append(temp.data.filled(0))\n",
        "\n",
        "    except:\n",
        "        print(\"An exception occurred\")\n",
        "\n",
        "tas_array_train = np.concatenate(tas_array_train)\n",
        "pr_array_train = np.concatenate(pr_array_train)\n",
        "gpp_array_train = np.concatenate(gpp_array_train)\n",
        "\n",
        "gpp_array_train = np.subtract(gpp_array_train, gpp_array_train.min()) / np.subtract(gpp_array_train.max(),\n",
        "                                                                                    gpp_array_train.min())\n",
        "\n",
        "pr_array_train = np.subtract(pr_array_train, pr_array_train.min()) / np.subtract(pr_array_train.max(),\n",
        "                                                                                 pr_array_train.min())\n",
        "\n",
        "tas_array_train = np.subtract(tas_array_train, tas_array_train.min()) / np.subtract(tas_array_train.max(),\n",
        "                                                                                    tas_array_train.min())\n",
        "\n",
        "mask_train = np.concatenate(mask_train)\n",
        "\n",
        "tas_array_test = np.concatenate(tas_array_test)\n",
        "pr_array_test = np.concatenate(pr_array_test)\n",
        "gpp_array_test = np.concatenate(gpp_array_test)\n",
        "\n",
        "tas_array_test = np.subtract(tas_array_test, tas_array_test.min()) / np.subtract(tas_array_test.max(),\n",
        "                                                                                 tas_array_test.min())\n",
        "\n",
        "pr_array_test = np.subtract(pr_array_test, pr_array_test.min()) / np.subtract(pr_array_test.max(), pr_array_test.min())\n",
        "\n",
        "gpp_test_min = gpp_array_test.min()\n",
        "gpp_test_max = gpp_array_test.max()\n",
        "\n",
        "gpp_array_test = np.subtract(gpp_array_test, gpp_array_test.min()) / np.subtract(gpp_array_test.max(),\n",
        "                                                                                 gpp_array_test.min())\n",
        "\n",
        "mask_test = np.concatenate(mask_test)\n",
        "train_data = np.stack([tas_array_train,pr_array_train], axis=-1)\n",
        "\n",
        "test_data = np.stack([tas_array_test,pr_array_test], axis=-1)\n",
        "\n",
        "model = tf.keras.models.load_model(model_path)\n",
        "# model.summary\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "z = model({\"inputs\": test_data})\n",
        "val = z.numpy()\n",
        "val1 = np.reshape(val, (test_data.shape[0], test_data.shape[1], test_data.shape[2]))\n",
        "finale_array = val1 * gpp_test_max\n",
        "\n",
        "import numpy.ma as ma\n",
        "# finale_array[finale_array<0]=0\n",
        "finale_array=ma.masked_array(finale_array,mask_test)\n",
        "import matplotlib.cm as mpl_cm\n",
        "\n",
        "brewer_cmap = mpl_cm.get_cmap('brewer_PiYG_11')\n",
        "\n",
        "\n",
        "\n",
        "start=0\n",
        "input_count=1\n",
        "outputcount=1\n",
        "for z in cubes_tas:\n",
        "    # try:\n",
        "        if (z._names[0] == 'gross_primary_productivity_of_biomass_expressed_as_carbon'):\n",
        "            temp = z.extract(time_constraint_test)\n",
        "            temp1=temp.copy()\n",
        "            temp1.data=finale_array[start:(start+temp.shape[0]),:,:]\n",
        "            start+=temp.shape[0]\n",
        "            for x,y in zip(temp.slices_over('time'),temp1.slices_over('time')):\n",
        "                fig = plt.figure(figsize=(20, 5),dpi=200)\n",
        "                plt.subplot(131)\n",
        "                qplt.contourf(x,brewer_cmap.N, cmap=brewer_cmap)\n",
        "                plt.title('GPP  '+str(x.coord('year').points[0]))\n",
        "                \n",
        "                \n",
        "                #plt.show()\n",
        "                plt.subplot(132)\n",
        "                qplt.contourf(y,brewer_cmap.N, cmap=brewer_cmap)\n",
        "                plt.title('Predicted GPP '+str(x.coord('year').points[0]))\n",
        "                \n",
        "                \n",
        "\n",
        "                plt.subplot(133)\n",
        "                qplt.contourf(x-y,brewer_cmap.N, cmap=brewer_cmap)\n",
        "                plt.title('Difference'+str(x.coord('year').points[0]))\n",
        "                # plt.show()\n",
        "                plt.savefig(plot_dir +\n",
        "                            'input'+str(input_count) )\n",
        "                plt.close('all')\n",
        "                input_count+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFuIJxDDHR7U"
      },
      "source": [
        "Model plots using tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fs8cDvqaHac6"
      },
      "source": [
        "# !pip install tensorboard\n",
        "%load_ext tensorboard\n",
        "import tensorflow as tf\n",
        "import datetime, os\n",
        "%tensorboard --logdir /content/drive/MyDrive/models_new/log"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}